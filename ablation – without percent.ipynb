{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Efficient Net download\n",
    "Install efficientnet downloaded from github. If using Kaggle, please switch on internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/qubvel/efficientnet\n",
      "  Cloning https://github.com/qubvel/efficientnet to /tmp/pip-req-build-jpa30wlk\n",
      "Collecting keras_applications<=1.0.8,>=1.0.7\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.1) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.18.5)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.4.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (3.2.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.4)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (7.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.8.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.14.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.10.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.1) (4.4.2)\n",
      "Building wheels for collected packages: efficientnet\n",
      "  Building wheel for efficientnet (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet: filename=efficientnet-1.1.1-py3-none-any.whl size=18420 sha256=3e9327974b038e78006d0957fd2891e38dcd25e61a520cef9399f36ae2718f5d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q0dk2wrn/wheels/11/69/85/814d64d694c96db0eef17b718042d644a1e54f113920481920\n",
      "Successfully built efficientnet\n",
      "Installing collected packages: keras-applications, efficientnet\n",
      "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# output is hidden in view version\n",
    "!pip3 install git+https://github.com/qubvel/efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import packages\n",
    "<span style = \"color:red\"> Could we remove the imports that aren't being used? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAABICAYAAADLcuPOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAACJElEQVR4nO3bsWqTYRiG4ffT1gxutUKdtLubno24/qehx+HqcQgubh0dHByFFlFURKWp+Lm4CMEYyMOX/FzXFvIPz0uHOyS09d4LALbt2ugBAMyTwAAQITAARAgMABECA0CEwAAQcbDugdbaVFVTVdXhYvHg6M5JfNQoB4vD0ROirt9Y++fea7euPo6ekHN5NXpB1PLrr9ETon4u5/tZ/uLH9/q8vGyr3mub/B/Myem9/vjpk60N2zVHp8ejJ0TN/b5H589HT4jpb9+PnhB1/vLb6AlRF+9ujp4QM716UW++fFoZmPlmFYChBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIgQGAAiBAaACIEBIEJgAIhovfd/P9DaVFXTn5f3q+p1etRAx1X1YfSIkDnfVuW+fee+/XW393571RtrA/PXw62d9d4fbm3WjpnzfXO+rcp9+8598+QrMgAiBAaAiE0D8yyyYnfM+b4531blvn3nvhna6DcYAPhfviIDIEJgAIgQGAAiBAaACIEBIOI3qVJbtUCORLYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Regular Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm \n",
    "from PIL import Image\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "import seaborn as sns\n",
    "import math\n",
    "import cv2\n",
    "import pydicom\n",
    "import os\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import matplotlib.image as mpimg\n",
    "from tabulate import tabulate\n",
    "import missingno as msno \n",
    "from IPython.display import display_html\n",
    "from PIL import Image\n",
    "import gc\n",
    "from skimage.transform import resize\n",
    "import copy\n",
    "import re\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Segmentation\n",
    "import glob\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import scipy.ndimage\n",
    "from skimage import morphology\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "from sklearn.cluster import KMeans\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from plotly.graph_objs import *\n",
    "init_notebook_mode(connected=True) \n",
    "\n",
    "# Model imports\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import (\n",
    "                                    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D,\n",
    "                                    Add, Conv2D, AveragePooling2D, LeakyReLU, Concatenate , Lambda\n",
    "                                    )\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.models as M\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow.keras.backend as K\n",
    "# import tensorflow.keras.applications as tfa\n",
    "import tensorflow_addons as tfa\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "custom_colors = ['#74a09e','#86c1b2','#98e2c6','#f3c969','#f2a553', '#d96548', '#c14953']\n",
    "sns.palplot(sns.color_palette(custom_colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tuned Hyperparameters\n",
    "We define the number of -\n",
    "* **epochs** : the number of times we loop through the dataset. \n",
    "* **batch_size**: how many training examples to feed into network before updating the weights and internal nodes\n",
    "* **LR**: learning rate\n",
    "* **MODEL_CLASS**:  allow us to define which efficientnet model we want to use\n",
    "* **SAVE_BEST**: defined as true, to only save the model due to 'early stopping' implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 8\n",
    "NFOLD = 5\n",
    "LR = 0.003\n",
    "SAVE_BEST = True\n",
    "MODEL_CLASS = 'b1'\n",
    "path = '../input/osic-pulmonary-fibrosis-progression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test split\n",
    "We create a test dataset with 20% of unique patients and their associated data; 80% for training. \n",
    "Duplicates are dropped as some patients have more than one recorded FVC value for each week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(f'{path}/train.csv') \n",
    "all_data.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n",
    "unique_patients = all_data.Patient.unique()\n",
    "train_ids, test_ids = train_test_split(unique_patients, test_size=0.2, random_state=42)\n",
    "train = all_data[all_data['Patient'].isin(train_ids)]\n",
    "test = all_data[all_data['Patient'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "The original features of the data included Age, Smoking_Status, Sex, Weeks, Percent. However, we would like to transform the data, and include altered features. \n",
    "\n",
    "The features that will be fed into the model eventually include:\n",
    "* baseline Age (numerical)\n",
    "* baseline Percent (numerical)\n",
    "* Gender (encoded)\n",
    "* Smoking status (encoded)\n",
    "\n",
    "Along with that, the numerical data is normalised and categorical variables are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_week(df):    \n",
    "    # make a copy to not change original df    \n",
    "    _df = df.copy()\n",
    "    # ensure all Weeks values are INT and not accidentaly saved as string\n",
    "    _df['Weeks'] = _df['Weeks'].astype(int)\n",
    "    _df['min_week'] = _df['Weeks']\n",
    "    _df[\"min_week\"] = _df.groupby('Patient')['Weeks'].transform('min')\n",
    "    _df['baselined_week'] = _df['Weeks'] - _df['min_week']\n",
    "    \n",
    "    return pd.DataFrame(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_baseline_week(train)\n",
    "#train = get_baseline_FVC_new(train)\n",
    "test = get_baseline_week(test)\n",
    "#test = get_baseline_FVC_new(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which attributes shall not be transformed, are numeric or categorical\n",
    "no_transform_attribs = ['Patient', 'Weeks', 'min_week', 'FVC']\n",
    "num_attribs = ['Percent', 'Age']\n",
    "cat_attribs = ['Sex', 'SmokingStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "num_attribs_scld = [s + '_scld' for s in num_attribs]\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train[num_attribs_scld] = min_max_scaler.fit_transform(train[num_attribs])\n",
    "test[num_attribs_scld] = min_max_scaler.transform(test[num_attribs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "      <th>min_week</th>\n",
       "      <th>baselined_week</th>\n",
       "      <th>Percent_scld</th>\n",
       "      <th>Age_scld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202489</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>55.712129</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.181129</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>51.862104</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.148772</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>53.950679</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.166325</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>52.063412</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-4</td>\n",
       "      <td>15</td>\n",
       "      <td>0.150464</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus  \\\n",
       "0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker   \n",
       "1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker   \n",
       "2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker   \n",
       "3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker   \n",
       "4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker   \n",
       "\n",
       "   min_week  baselined_week  Percent_scld  Age_scld  \n",
       "0        -4               0      0.202489  0.789474  \n",
       "1        -4               9      0.181129  0.789474  \n",
       "2        -4              11      0.148772  0.789474  \n",
       "3        -4              13      0.166325  0.789474  \n",
       "4        -4              15      0.150464  0.789474  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "      <th>min_week</th>\n",
       "      <th>baselined_week</th>\n",
       "      <th>Percent_scld</th>\n",
       "      <th>Age_scld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>-3</td>\n",
       "      <td>1536</td>\n",
       "      <td>65.306122</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261761</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>3</td>\n",
       "      <td>1368</td>\n",
       "      <td>58.163265</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.201730</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>5</td>\n",
       "      <td>1361</td>\n",
       "      <td>57.865646</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.199228</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>7</td>\n",
       "      <td>1465</td>\n",
       "      <td>62.287415</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.236391</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>9</td>\n",
       "      <td>1681</td>\n",
       "      <td>71.471088</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.313574</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Patient  Weeks   FVC    Percent  Age     Sex  \\\n",
       "80  ID00023637202179104603099     -3  1536  65.306122   71  Female   \n",
       "81  ID00023637202179104603099      3  1368  58.163265   71  Female   \n",
       "82  ID00023637202179104603099      5  1361  57.865646   71  Female   \n",
       "83  ID00023637202179104603099      7  1465  62.287415   71  Female   \n",
       "84  ID00023637202179104603099      9  1681  71.471088   71  Female   \n",
       "\n",
       "   SmokingStatus  min_week  baselined_week  Percent_scld  Age_scld  \n",
       "80     Ex-smoker        -3               0      0.261761  0.578947  \n",
       "81     Ex-smoker        -3               6      0.201730  0.578947  \n",
       "82     Ex-smoker        -3               8      0.199228  0.578947  \n",
       "83     Ex-smoker        -3              10      0.236391  0.578947  \n",
       "84     Ex-smoker        -3              12      0.313574  0.578947  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding our categorical variables. \n",
    "* Gender: Male: 0, Female: 1\n",
    "* Smoking_Status: Never Smoked: [0,0], Ex-Smoker: [1,1], Currently Smokes: [0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tab_scaled(df): #getting scaled variables\n",
    "    vector = [df.Age_scld.values[0]] # only the first percent value is retained, so this forms the \"base_percent\"\n",
    "    if df.Sex.values[0].lower() == 'male':\n",
    "        vector.append(0)\n",
    "    else:\n",
    "        vector.append(1)\n",
    "    \n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0,0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1,1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0,1])\n",
    "    else:\n",
    "        vector.extend([1,0]) # what is the purpose of this, there arent any other values smoking_status can take?\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the labels i.e y values for our dataset. We aim to predict the rate of FVC values deterioration, which would be the gradient of a linear regression model fit to every available patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295ae44005b44bcc832401937ee4e9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "A = {} # Gradient of the linear regression of FVC against weeks \n",
    "TAB = {} # Initialize tabular data for each patient\n",
    "P = [] # Patient IDs\n",
    "\n",
    "# for all 140 train patients we compute the gradient \n",
    "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
    "    sub = train.loc[train.Patient == p, :] \n",
    "    fvc = sub.FVC.values    \n",
    "    weeks = sub.Weeks.values \n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
    "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
    "    \n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab_scaled(sub)\n",
    "    # TAB[p] = get_tab_unscaled(sub)\n",
    "    P.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sliced ct scan, we need to resize into 512x512 pixel dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    d = pydicom.dcmread(path)\n",
    "    return cv2.resize((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope * 1000), (512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are patients (labelled with BAD_IDs ( <span style = \"color:red\"> why are they bad_IDs maybe we need to explain that ? </span> )) that need to be removed from the training data. This class also transforms the dataframes into the required numpy arrays so that our data can be input into the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGenerator(Sequence):\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab, batch_size=BATCH_SIZE):\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID] # all the patients in the keys  \n",
    "        self.a = a # gradients found by fitting a linear regression model\n",
    "        self.tab = tab # tabular data for patient specified in keys\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_data = {} # initialise a dictionary to contain all the images pertaining to one key: patient id \n",
    "        for p in train.Patient.values:\n",
    "            self.train_data[p] = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000 # 1000 batches per epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        keys = np.random.choice(self.keys, size = self.batch_size) # randomly chosen n patients for one batch  \n",
    "        for k in keys:\n",
    "            try:\n",
    "                i = np.random.choice(self.train_data[k], size=1)[0]\n",
    "                img = get_img(f'../input/osic-pulmonary-fibrosis-progression/train/{k}/{i}')\n",
    "                x.append(img)\n",
    "                a.append(self.a[k])\n",
    "                tab.append(self.tab[k])\n",
    "            except:\n",
    "                print(k, i)\n",
    "       \n",
    "        x,a,tab = np.array(x), np.array(a), np.array(tab)\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        return [x, tab] , a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficientnet(model, shape):\n",
    "    '''\n",
    "    From https://github.com/qubvel/efficientnet\n",
    "    EfficientNet is a CNN architecture achieving state of the art accuracy.\n",
    "    b0 is the simplest model, b7 is the most complex.\n",
    "    '''\n",
    "    models_dict = {\n",
    "        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n",
    "        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False), # We use a b1 efficientnet\n",
    "        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n",
    "        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n",
    "        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n",
    "        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n",
    "        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n",
    "        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n",
    "    }\n",
    "    return models_dict[model]\n",
    "\n",
    "def build_model(shape=(512, 512, 1), model_class=None):\n",
    "    inp = Input(shape=shape) # 512 x 512 input shape\n",
    "    base = get_efficientnet(model_class, shape) # A b1 pre-trained efficientnet is used\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    inp2 = Input(shape=(4,))\n",
    "    out_tab = tf.keras.layers.GaussianNoise(0.2)(inp2) # add some noise to our data\n",
    "    \n",
    "    x2 = Concatenate()([x, out_tab]) \n",
    "    \n",
    "    #### added ###\n",
    "    den_1 = Dense(20)(x2) # linear layer with 50 output nodes\n",
    "    den_1 = LeakyReLU(alpha=0.3)(den_1) # default alpha is 0.3\n",
    "    den_2 = Dense(500)(den_1)\n",
    "    den_2 = LeakyReLU(alpha=0.3)(den_2)\n",
    "    bn_1 = BatchNormalization()(den_2)\n",
    "    den_3 = Dense(100)(bn_1)\n",
    "    \n",
    "    x3 = Dropout(0.5)(den_3) # move the dropout layer to before the linear layers \n",
    "    \n",
    "    # the linear layers here are akin to the final dense layer usually used in a \"before concat\" model\n",
    "    out_1 = Dense(1)(x3)\n",
    "    out_2 = Dense(1, activation='relu')(x3)\n",
    "    y = out_1 + tf.keras.backend.cumsum(out_2, axis=1)\n",
    "    \n",
    "    model = Model([inp, inp2], y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "\n",
    "We split the dataset into 5 folds for cross validation purposes. There is early stopping employed in the model and the loss function that we decided upon is RMSE = Root Mean Squared Error. It is a commonly used loss function in regression problems.  <span style = \"color:red\"> do we need to change mse in the code to rmse ? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "####### Fold 0 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 70.1550\n",
      "Epoch 00001: val_loss improved from inf to 79983440.00000, saving model to fold-0.h5\n",
      "32/32 [==============================] - 18s 555ms/step - loss: 70.1550 - val_loss: 79983440.0000\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 54.8713\n",
      "Epoch 00002: val_loss improved from 79983440.00000 to 143744.56250, saving model to fold-0.h5\n",
      "32/32 [==============================] - 16s 487ms/step - loss: 54.8713 - val_loss: 143744.5625\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 39.4601\n",
      "Epoch 00003: val_loss improved from 143744.56250 to 497.31488, saving model to fold-0.h5\n",
      "32/32 [==============================] - 16s 496ms/step - loss: 39.4601 - val_loss: 497.3149\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 46.7848\n",
      "Epoch 00004: val_loss did not improve from 497.31488\n",
      "32/32 [==============================] - 15s 454ms/step - loss: 46.7848 - val_loss: 215352.1562\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 47.7634\n",
      "Epoch 00005: val_loss did not improve from 497.31488\n",
      "32/32 [==============================] - 15s 457ms/step - loss: 47.7634 - val_loss: 1837170432.0000\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 59.1922\n",
      "Epoch 00006: val_loss did not improve from 497.31488\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 59.1922 - val_loss: 1506346.0000\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 45.1893\n",
      "Epoch 00007: val_loss improved from 497.31488 to 134.45825, saving model to fold-0.h5\n",
      "32/32 [==============================] - 16s 504ms/step - loss: 45.1893 - val_loss: 134.4583\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 47.5473\n",
      "Epoch 00008: val_loss did not improve from 134.45825\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 47.5473 - val_loss: 11699.3682\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.1897\n",
      "Epoch 00009: val_loss improved from 134.45825 to 32.89404, saving model to fold-0.h5\n",
      "32/32 [==============================] - 16s 495ms/step - loss: 36.1897 - val_loss: 32.8940\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.2721\n",
      "Epoch 00010: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 33.2721 - val_loss: 33.0219\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.6045\n",
      "Epoch 00011: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 15s 476ms/step - loss: 34.6045 - val_loss: 49.9206\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 44.9330\n",
      "Epoch 00012: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 44.9330 - val_loss: 126.2141\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 40.1422\n",
      "Epoch 00013: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 40.1422 - val_loss: 35.6513\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.3463\n",
      "Epoch 00014: val_loss did not improve from 32.89404\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 15s 457ms/step - loss: 31.3463 - val_loss: 479.8203\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.5044\n",
      "Epoch 00015: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 15s 455ms/step - loss: 36.5044 - val_loss: 39.8189\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.9143\n",
      "Epoch 00016: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 34.9143 - val_loss: 34.0664\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.3798\n",
      "Epoch 00017: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 30.3798 - val_loss: 55.7424\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.1045\n",
      "Epoch 00018: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 30.1045 - val_loss: 48.8803\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.7681\n",
      "Epoch 00019: val_loss did not improve from 32.89404\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 36.7681 - val_loss: 36.7337\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.8631\n",
      "Epoch 00020: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 29.8631 - val_loss: 43.1261\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.4517\n",
      "Epoch 00021: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 36.4517 - val_loss: 38.7989\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 40.7003\n",
      "Epoch 00022: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 40.7003 - val_loss: 41.9806\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.8024\n",
      "Epoch 00023: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 32.8024 - val_loss: 34.5065\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 40.7319\n",
      "Epoch 00024: val_loss did not improve from 32.89404\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 40.7319 - val_loss: 42.0071\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 28.4898\n",
      "Epoch 00025: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 28.4898 - val_loss: 49.3346\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.7737\n",
      "Epoch 00026: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 29.7737 - val_loss: 46.9735\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.2529\n",
      "Epoch 00027: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 15s 456ms/step - loss: 33.2529 - val_loss: 46.6820\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 42.9306\n",
      "Epoch 00028: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 453ms/step - loss: 42.9306 - val_loss: 41.1841\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.4836\n",
      "Epoch 00029: val_loss did not improve from 32.89404\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 33.4836 - val_loss: 47.1634\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.9944\n",
      "Epoch 00030: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 33.9944 - val_loss: 46.3370\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.4101\n",
      "Epoch 00031: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 15s 456ms/step - loss: 37.4101 - val_loss: 37.9889\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.5181\n",
      "Epoch 00032: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 31.5181 - val_loss: 42.6792\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.9969\n",
      "Epoch 00033: val_loss did not improve from 32.89404\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 29.9969 - val_loss: 37.1879\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 28.9077\n",
      "Epoch 00034: val_loss did not improve from 32.89404\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 28.9077 - val_loss: 36.8185\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.9687\n",
      "Epoch 00035: val_loss improved from 32.89404 to 32.44246, saving model to fold-0.h5\n",
      "32/32 [==============================] - 16s 496ms/step - loss: 27.9687 - val_loss: 32.4425\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.2206\n",
      "Epoch 00036: val_loss did not improve from 32.44246\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 37.2206 - val_loss: 40.7665\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.1331\n",
      "Epoch 00037: val_loss did not improve from 32.44246\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 29.1331 - val_loss: 33.8958\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 26.2209\n",
      "Epoch 00038: val_loss did not improve from 32.44246\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 26.2209 - val_loss: 40.6966\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.6052\n",
      "Epoch 00039: val_loss did not improve from 32.44246\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 30.6052 - val_loss: 46.2058\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.4585\n",
      "Epoch 00040: val_loss did not improve from 32.44246\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 29.4585 - val_loss: 34.3707\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 1 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 66.0597\n",
      "Epoch 00001: val_loss improved from inf to 51.52144, saving model to fold-1.h5\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 66.0597 - val_loss: 51.5214\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 64.9421\n",
      "Epoch 00002: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 64.9421 - val_loss: 10534136.0000\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 59.8631\n",
      "Epoch 00003: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 59.8631 - val_loss: 393194.4062\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 83.2872\n",
      "Epoch 00004: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 15s 458ms/step - loss: 83.2872 - val_loss: 4008533.5000\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 54.6313\n",
      "Epoch 00005: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 54.6313 - val_loss: 9990374.0000\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 52.8004\n",
      "Epoch 00006: val_loss did not improve from 51.52144\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 52.8004 - val_loss: 771118.1250\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 41.2772\n",
      "Epoch 00007: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 15s 455ms/step - loss: 41.2772 - val_loss: 145.6712\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 45.1876\n",
      "Epoch 00008: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 45.1876 - val_loss: 188.8678\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 50.8611\n",
      "Epoch 00009: val_loss did not improve from 51.52144\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 50.8611 - val_loss: 1312.0182\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 39.2911\n",
      "Epoch 00010: val_loss improved from 51.52144 to 27.34771, saving model to fold-1.h5\n",
      "32/32 [==============================] - 16s 487ms/step - loss: 39.2911 - val_loss: 27.3477\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.6368\n",
      "Epoch 00011: val_loss did not improve from 27.34771\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 36.6368 - val_loss: 83.7576\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 41.6040\n",
      "Epoch 00012: val_loss did not improve from 27.34771\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 41.6040 - val_loss: 37.6709\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 47.0387\n",
      "Epoch 00013: val_loss did not improve from 27.34771\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 47.0387 - val_loss: 129.4907\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 42.9245\n",
      "Epoch 00014: val_loss did not improve from 27.34771\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 42.9245 - val_loss: 151.2660\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 47.1368\n",
      "Epoch 00015: val_loss improved from 27.34771 to 21.48448, saving model to fold-1.h5\n",
      "32/32 [==============================] - 16s 499ms/step - loss: 47.1368 - val_loss: 21.4845\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 44.6720\n",
      "Epoch 00016: val_loss improved from 21.48448 to 17.74067, saving model to fold-1.h5\n",
      "32/32 [==============================] - 15s 479ms/step - loss: 44.6720 - val_loss: 17.7407\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.6440\n",
      "Epoch 00017: val_loss did not improve from 17.74067\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 31.6440 - val_loss: 22.2156\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.0872\n",
      "Epoch 00018: val_loss did not improve from 17.74067\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 38.0872 - val_loss: 319.8013\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.0210\n",
      "Epoch 00019: val_loss did not improve from 17.74067\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 37.0210 - val_loss: 20.6112\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.0925\n",
      "Epoch 00020: val_loss did not improve from 17.74067\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 38.0925 - val_loss: 20.2055\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 49.0545\n",
      "Epoch 00021: val_loss did not improve from 17.74067\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 49.0545 - val_loss: 18.7105\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.2271\n",
      "Epoch 00022: val_loss improved from 17.74067 to 17.72734, saving model to fold-1.h5\n",
      "32/32 [==============================] - 15s 477ms/step - loss: 36.2271 - val_loss: 17.7273\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 44.4947\n",
      "Epoch 00023: val_loss did not improve from 17.72734\n",
      "32/32 [==============================] - 15s 465ms/step - loss: 44.4947 - val_loss: 18.6798\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.7642\n",
      "Epoch 00024: val_loss did not improve from 17.72734\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 34.7642 - val_loss: 19.3747\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 28.4263\n",
      "Epoch 00025: val_loss did not improve from 17.72734\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 28.4263 - val_loss: 18.1665\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.6239\n",
      "Epoch 00026: val_loss improved from 17.72734 to 16.16131, saving model to fold-1.h5\n",
      "32/32 [==============================] - 15s 480ms/step - loss: 34.6239 - val_loss: 16.1613\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.6708\n",
      "Epoch 00027: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 15s 465ms/step - loss: 38.6708 - val_loss: 18.8488\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.7093\n",
      "Epoch 00028: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 36.7093 - val_loss: 21.2180\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.9947\n",
      "Epoch 00029: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 38.9947 - val_loss: 23.5436\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.8258\n",
      "Epoch 00030: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 37.8258 - val_loss: 18.6471\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.4847\n",
      "Epoch 00031: val_loss did not improve from 16.16131\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 27.4847 - val_loss: 19.6118\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.8941\n",
      "Epoch 00032: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 37.8941 - val_loss: 18.0683\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.3586\n",
      "Epoch 00033: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 35.3586 - val_loss: 17.0798\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.8618\n",
      "Epoch 00034: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 34.8618 - val_loss: 18.0785\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.7297\n",
      "Epoch 00035: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 31.7297 - val_loss: 18.8423\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.5907\n",
      "Epoch 00036: val_loss did not improve from 16.16131\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 35.5907 - val_loss: 20.0532\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.3634\n",
      "Epoch 00037: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 35.3634 - val_loss: 19.5613\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.4583\n",
      "Epoch 00038: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 34.4583 - val_loss: 18.0857\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 43.5998\n",
      "Epoch 00039: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 15s 454ms/step - loss: 43.5998 - val_loss: 20.1183\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.1679\n",
      "Epoch 00040: val_loss did not improve from 16.16131\n",
      "32/32 [==============================] - 14s 453ms/step - loss: 33.1679 - val_loss: 20.2745\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 2 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 80.4267\n",
      "Epoch 00001: val_loss improved from inf to 28.04887, saving model to fold-2.h5\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 80.4267 - val_loss: 28.0489\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 51.3952\n",
      "Epoch 00002: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 51.3952 - val_loss: 31.8055\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 70.4761\n",
      "Epoch 00003: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 70.4761 - val_loss: 29.5476\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 89.6678\n",
      "Epoch 00004: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 15s 453ms/step - loss: 89.6678 - val_loss: 4809811968.0000\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 56.0858\n",
      "Epoch 00005: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 56.0858 - val_loss: 220966772801536.0000\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 46.5681\n",
      "Epoch 00006: val_loss did not improve from 28.04887\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 46.5681 - val_loss: 160667615232.0000\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.9979\n",
      "Epoch 00007: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 15s 466ms/step - loss: 38.9979 - val_loss: 742684.2500\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 45.2501\n",
      "Epoch 00008: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 45.2501 - val_loss: 75418.7500\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 39.8386\n",
      "Epoch 00009: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 39.8386 - val_loss: 34.9444\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 39.9483\n",
      "Epoch 00010: val_loss did not improve from 28.04887\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 39.9483 - val_loss: 16257.1885\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.8654\n",
      "Epoch 00011: val_loss did not improve from 28.04887\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 34.8654 - val_loss: 37.0063\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.6321\n",
      "Epoch 00012: val_loss improved from 28.04887 to 27.49034, saving model to fold-2.h5\n",
      "32/32 [==============================] - 15s 481ms/step - loss: 36.6321 - val_loss: 27.4903\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.7898\n",
      "Epoch 00013: val_loss did not improve from 27.49034\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 38.7898 - val_loss: 33.4997\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.2307\n",
      "Epoch 00014: val_loss did not improve from 27.49034\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 37.2307 - val_loss: 32.5411\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.7903\n",
      "Epoch 00015: val_loss did not improve from 27.49034\n",
      "32/32 [==============================] - 15s 456ms/step - loss: 37.7903 - val_loss: 40.0399\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.4287\n",
      "Epoch 00016: val_loss did not improve from 27.49034\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 33.4287 - val_loss: 35.9804\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.3264\n",
      "Epoch 00017: val_loss improved from 27.49034 to 26.83473, saving model to fold-2.h5\n",
      "32/32 [==============================] - 15s 477ms/step - loss: 35.3264 - val_loss: 26.8347\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.7259\n",
      "Epoch 00018: val_loss did not improve from 26.83473\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 37.7259 - val_loss: 35.3522\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.5158\n",
      "Epoch 00019: val_loss improved from 26.83473 to 25.88027, saving model to fold-2.h5\n",
      "32/32 [==============================] - 16s 489ms/step - loss: 33.5158 - val_loss: 25.8803\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.9293\n",
      "Epoch 00020: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 27.9293 - val_loss: 32.0081\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.1119\n",
      "Epoch 00021: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 30.1119 - val_loss: 27.7662\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.9319\n",
      "Epoch 00022: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 29.9319 - val_loss: 27.8238\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.7057\n",
      "Epoch 00023: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 32.7057 - val_loss: 40.9493\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 25.4158\n",
      "Epoch 00024: val_loss did not improve from 25.88027\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 25.4158 - val_loss: 36.9688\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.9089\n",
      "Epoch 00025: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 27.9089 - val_loss: 38.8022\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.1976\n",
      "Epoch 00026: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 34.1976 - val_loss: 34.8290\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.0440\n",
      "Epoch 00027: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 15s 455ms/step - loss: 29.0440 - val_loss: 42.7637\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.9467\n",
      "Epoch 00028: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 27.9467 - val_loss: 35.7892\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.2017\n",
      "Epoch 00029: val_loss did not improve from 25.88027\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 15s 454ms/step - loss: 27.2017 - val_loss: 31.8713\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.8556\n",
      "Epoch 00030: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 32.8556 - val_loss: 28.6866\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.7691\n",
      "Epoch 00031: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 29.7691 - val_loss: 35.5317\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.1624\n",
      "Epoch 00032: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 31.1624 - val_loss: 48.1152\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 24.9570\n",
      "Epoch 00033: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 24.9570 - val_loss: 33.2510\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.2238\n",
      "Epoch 00034: val_loss did not improve from 25.88027\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 29.2238 - val_loss: 46.3297\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.5303\n",
      "Epoch 00035: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 33.5303 - val_loss: 38.1189\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.0301\n",
      "Epoch 00036: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 33.0301 - val_loss: 42.8273\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.8352\n",
      "Epoch 00037: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 34.8352 - val_loss: 30.3400\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.9775\n",
      "Epoch 00038: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 37.9775 - val_loss: 39.8139\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 23.6031\n",
      "Epoch 00039: val_loss did not improve from 25.88027\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 23.6031 - val_loss: 32.8288\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.8754\n",
      "Epoch 00040: val_loss did not improve from 25.88027\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 32.8754 - val_loss: 48.2311\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 3 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 93.6528\n",
      "Epoch 00001: val_loss improved from inf to 193.89485, saving model to fold-3.h5\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 93.6528 - val_loss: 193.8949\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 72.2298\n",
      "Epoch 00002: val_loss improved from 193.89485 to 61.66386, saving model to fold-3.h5\n",
      "32/32 [==============================] - 15s 474ms/step - loss: 72.2298 - val_loss: 61.6639\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 45.8359\n",
      "Epoch 00003: val_loss did not improve from 61.66386\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 45.8359 - val_loss: 16031.6064\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 49.1313\n",
      "Epoch 00004: val_loss improved from 61.66386 to 27.81023, saving model to fold-3.h5\n",
      "32/32 [==============================] - 15s 483ms/step - loss: 49.1313 - val_loss: 27.8102\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 45.7817\n",
      "Epoch 00005: val_loss did not improve from 27.81023\n",
      "32/32 [==============================] - 14s 438ms/step - loss: 45.7817 - val_loss: 1394.3817\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 48.2907\n",
      "Epoch 00006: val_loss did not improve from 27.81023\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 48.2907 - val_loss: 181796.7656\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.0769\n",
      "Epoch 00007: val_loss did not improve from 27.81023\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 35.0769 - val_loss: 3880.2754\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 42.4907\n",
      "Epoch 00008: val_loss did not improve from 27.81023\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 42.4907 - val_loss: 12121.4131\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 40.7963\n",
      "Epoch 00009: val_loss did not improve from 27.81023\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 40.7963 - val_loss: 422692.5938\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.5076\n",
      "Epoch 00010: val_loss did not improve from 27.81023\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 33.5076 - val_loss: 4140.2744\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.5381\n",
      "Epoch 00011: val_loss did not improve from 27.81023\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 38.5381 - val_loss: 32.2159\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.6664\n",
      "Epoch 00012: val_loss improved from 27.81023 to 27.57673, saving model to fold-3.h5\n",
      "32/32 [==============================] - 16s 495ms/step - loss: 33.6664 - val_loss: 27.5767\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.9113\n",
      "Epoch 00013: val_loss improved from 27.57673 to 17.30134, saving model to fold-3.h5\n",
      "32/32 [==============================] - 15s 471ms/step - loss: 29.9113 - val_loss: 17.3013\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.7116\n",
      "Epoch 00014: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 32.7116 - val_loss: 34.4818\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.2440\n",
      "Epoch 00015: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 32.2440 - val_loss: 41.2690\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.8959\n",
      "Epoch 00016: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 32.8959 - val_loss: 28.1428\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.3540\n",
      "Epoch 00017: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 37.3540 - val_loss: 31.9135\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.0146\n",
      "Epoch 00018: val_loss did not improve from 17.30134\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 34.0146 - val_loss: 32.5504\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.1079\n",
      "Epoch 00019: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 36.1079 - val_loss: 25.9152\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.1028\n",
      "Epoch 00020: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 31.1028 - val_loss: 28.2553\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.1295\n",
      "Epoch 00021: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 27.1295 - val_loss: 48.7818\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.3528\n",
      "Epoch 00022: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 30.3528 - val_loss: 29.1846\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.0702\n",
      "Epoch 00023: val_loss did not improve from 17.30134\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 30.0702 - val_loss: 39.4532\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.1785\n",
      "Epoch 00024: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 32.1785 - val_loss: 32.0823\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 43.4725\n",
      "Epoch 00025: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 438ms/step - loss: 43.4725 - val_loss: 25.2038\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.1861\n",
      "Epoch 00026: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 434ms/step - loss: 32.1861 - val_loss: 26.5506\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.1928\n",
      "Epoch 00027: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 439ms/step - loss: 31.1928 - val_loss: 26.6412\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.0331\n",
      "Epoch 00028: val_loss did not improve from 17.30134\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 32.0331 - val_loss: 28.8344\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.3779\n",
      "Epoch 00029: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 30.3779 - val_loss: 28.7902\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.6894\n",
      "Epoch 00030: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 33.6894 - val_loss: 29.2534\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.8775\n",
      "Epoch 00031: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 32.8775 - val_loss: 33.1555\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.6102\n",
      "Epoch 00032: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 32.6102 - val_loss: 27.8775\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.3628\n",
      "Epoch 00033: val_loss did not improve from 17.30134\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 14s 439ms/step - loss: 30.3628 - val_loss: 28.2932\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 26.4767\n",
      "Epoch 00034: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 26.4767 - val_loss: 31.5044\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.9507\n",
      "Epoch 00035: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 439ms/step - loss: 32.9507 - val_loss: 30.8086\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 24.9467\n",
      "Epoch 00036: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 24.9467 - val_loss: 27.8918\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.0546\n",
      "Epoch 00037: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 437ms/step - loss: 30.0546 - val_loss: 43.5446\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.3933\n",
      "Epoch 00038: val_loss did not improve from 17.30134\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 33.3933 - val_loss: 32.3998\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 26.7879\n",
      "Epoch 00039: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 26.7879 - val_loss: 26.4764\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.6418\n",
      "Epoch 00040: val_loss did not improve from 17.30134\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 32.6418 - val_loss: 26.5093\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 4 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 91.0022\n",
      "Epoch 00001: val_loss improved from inf to 81.04678, saving model to fold-4.h5\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 91.0022 - val_loss: 81.0468\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 45.6075\n",
      "Epoch 00002: val_loss improved from 81.04678 to 27.28998, saving model to fold-4.h5\n",
      "32/32 [==============================] - 15s 477ms/step - loss: 45.6075 - val_loss: 27.2900\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 53.7027\n",
      "Epoch 00003: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 53.7027 - val_loss: 43.0422\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 74.4190\n",
      "Epoch 00004: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 74.4190 - val_loss: 37.0198\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 55.4250\n",
      "Epoch 00005: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 55.4250 - val_loss: 140224.4688\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 52.1587\n",
      "Epoch 00006: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 52.1587 - val_loss: 25539480.0000\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 52.7635\n",
      "Epoch 00007: val_loss did not improve from 27.28998\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 14s 453ms/step - loss: 52.7635 - val_loss: 1249909.0000\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 41.3747\n",
      "Epoch 00008: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 41.3747 - val_loss: 10565.7773\n",
      "Epoch 9/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.7380\n",
      "Epoch 00009: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 38.7380 - val_loss: 70.2657\n",
      "Epoch 10/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 31.9495\n",
      "Epoch 00010: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 31.9495 - val_loss: 48.1694\n",
      "Epoch 11/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.3458\n",
      "Epoch 00011: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 38.3458 - val_loss: 41.8036\n",
      "Epoch 12/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.8305\n",
      "Epoch 00012: val_loss did not improve from 27.28998\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 34.8305 - val_loss: 49.5675\n",
      "Epoch 13/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.8866\n",
      "Epoch 00013: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 15s 453ms/step - loss: 36.8866 - val_loss: 36.7112\n",
      "Epoch 14/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.0970\n",
      "Epoch 00014: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 440ms/step - loss: 37.0970 - val_loss: 36.8194\n",
      "Epoch 15/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 26.1366\n",
      "Epoch 00015: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 15s 458ms/step - loss: 26.1366 - val_loss: 40.5336\n",
      "Epoch 16/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 30.4795\n",
      "Epoch 00016: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 450ms/step - loss: 30.4795 - val_loss: 27.6470\n",
      "Epoch 17/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.5909\n",
      "Epoch 00017: val_loss did not improve from 27.28998\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 29.5909 - val_loss: 34.9407\n",
      "Epoch 18/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 37.7635\n",
      "Epoch 00018: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 37.7635 - val_loss: 41.1695\n",
      "Epoch 19/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 29.9521\n",
      "Epoch 00019: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 29.9521 - val_loss: 44.6915\n",
      "Epoch 20/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 28.2242\n",
      "Epoch 00020: val_loss did not improve from 27.28998\n",
      "32/32 [==============================] - 14s 449ms/step - loss: 28.2242 - val_loss: 31.4516\n",
      "Epoch 21/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 26.4140\n",
      "Epoch 00021: val_loss improved from 27.28998 to 23.61503, saving model to fold-4.h5\n",
      "32/32 [==============================] - 15s 481ms/step - loss: 26.4140 - val_loss: 23.6150\n",
      "Epoch 22/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.5375\n",
      "Epoch 00022: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 34.5375 - val_loss: 28.2143\n",
      "Epoch 23/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.4423\n",
      "Epoch 00023: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 453ms/step - loss: 36.4423 - val_loss: 30.8596\n",
      "Epoch 24/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 28.8926\n",
      "Epoch 00024: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 451ms/step - loss: 28.8926 - val_loss: 39.1590\n",
      "Epoch 25/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.3078\n",
      "Epoch 00025: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 33.3078 - val_loss: 30.8380\n",
      "Epoch 26/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.1359\n",
      "Epoch 00026: val_loss did not improve from 23.61503\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 32.1359 - val_loss: 25.5842\n",
      "Epoch 27/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.2812\n",
      "Epoch 00027: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 34.2812 - val_loss: 36.0330\n",
      "Epoch 28/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 24.1463\n",
      "Epoch 00028: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 447ms/step - loss: 24.1463 - val_loss: 27.9687\n",
      "Epoch 29/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 26.9818\n",
      "Epoch 00029: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 26.9818 - val_loss: 25.5244\n",
      "Epoch 30/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.2228\n",
      "Epoch 00030: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 32.2228 - val_loss: 32.5882\n",
      "Epoch 31/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 33.6836\n",
      "Epoch 00031: val_loss did not improve from 23.61503\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 33.6836 - val_loss: 28.9076\n",
      "Epoch 32/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 34.4173\n",
      "Epoch 00032: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 443ms/step - loss: 34.4173 - val_loss: 43.1744\n",
      "Epoch 33/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.5407\n",
      "Epoch 00033: val_loss did not improve from 23.61503\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 32.5407 - val_loss: 25.8009\n",
      "Epoch 34/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 36.1138\n",
      "Epoch 00034: val_loss improved from 23.61503 to 22.34600, saving model to fold-4.h5\n",
      "32/32 [==============================] - 15s 477ms/step - loss: 36.1138 - val_loss: 22.3460\n",
      "Epoch 35/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 27.8709\n",
      "Epoch 00035: val_loss did not improve from 22.34600\n",
      "32/32 [==============================] - 14s 452ms/step - loss: 27.8709 - val_loss: 49.0537\n",
      "Epoch 36/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.6807\n",
      "Epoch 00036: val_loss did not improve from 22.34600\n",
      "32/32 [==============================] - 14s 446ms/step - loss: 35.6807 - val_loss: 30.0265\n",
      "Epoch 37/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 39.4850\n",
      "Epoch 00037: val_loss did not improve from 22.34600\n",
      "32/32 [==============================] - 14s 441ms/step - loss: 39.4850 - val_loss: 27.7293\n",
      "Epoch 38/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 38.6555\n",
      "Epoch 00038: val_loss did not improve from 22.34600\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 38.6555 - val_loss: 32.8478\n",
      "Epoch 39/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 28.9685\n",
      "Epoch 00039: val_loss did not improve from 22.34600\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 14s 444ms/step - loss: 28.9685 - val_loss: 32.7526\n",
      "Epoch 40/40\n",
      "32/32 [==============================] - ETA: 0s - loss: 35.3346\n",
      "Epoch 00040: val_loss did not improve from 22.34600\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 35.3346 - val_loss: 31.9854\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=NFOLD, random_state=42,shuffle=False)\n",
    "P = np.array(P)\n",
    "subs = []\n",
    "folds_history = []\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(P)):\n",
    "    print('#####################')\n",
    "    print('####### Fold %i ######'%fold)\n",
    "    print('#####################')\n",
    "    print('Training...')\n",
    "    \n",
    "    er = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-3,\n",
    "        patience=8,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    cpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='fold-%i.h5'%fold,\n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=SAVE_BEST,\n",
    "        mode='auto'\n",
    "    )\n",
    "\n",
    "    rlp = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5,\n",
    "        patience=5, \n",
    "        verbose=1, \n",
    "        min_lr=1e-8\n",
    "    )\n",
    "    model = build_model(model_class=MODEL_CLASS)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=\"mse\") \n",
    "    history = model.fit_generator(IGenerator(keys=P[tr_idx], \n",
    "                                   a = A, \n",
    "                                   tab = TAB), \n",
    "                        steps_per_epoch = 32,\n",
    "                        validation_data=IGenerator(keys=P[val_idx], \n",
    "                                   a = A, \n",
    "                                   tab = TAB),\n",
    "                        validation_steps = 16, \n",
    "                        callbacks = [cpt, rlp], \n",
    "                        epochs=EPOCHS)\n",
    "    folds_history.append(history.history)\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validation Loss examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32.44245529174805\n",
      "1 16.161312103271484\n",
      "2 25.880266189575195\n",
      "3 17.301340103149414\n",
      "4 22.345998764038086\n",
      "16.161312103271484\n"
     ]
    }
   ],
   "source": [
    "min_array = []\n",
    "for i in range(5):\n",
    "    min_array.append(min(folds_history[i]['val_loss']))\n",
    "    print(i, min(folds_history[i]['val_loss']))\n",
    "print(min(min_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our mean CV MAE is: 22.826274490356447\n"
     ]
    }
   ],
   "source": [
    "# We pick the best model (weights) based on cross validation score.\n",
    "if SAVE_BEST:\n",
    "    mean_val_loss = np.mean([np.min(h['val_loss']) for h in folds_history])\n",
    "else:\n",
    "    mean_val_loss = np.mean([h['val_loss'][-1] for h in folds_history])\n",
    "print('Our mean CV MAE is: ' + str(mean_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_fold finds out which fold gives the least validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_fold = np.argmin([np.min(h['val_loss']) for h in folds_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^print what is the fold that gives min loss, keep a record below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# min_fold = 1 #change accordingly\n",
    "################################\n",
    "#uncomment for using notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. download the weights from the output and save to zip file and upload them \n",
    "2. Restart kernel at this point for fitting weights from best fold to model in model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pred_model(shape=(512, 512, 1), model_class=None, fold=None):\n",
    "    inp = Input(shape=shape) # 512 x 512 input shape\n",
    "    base = get_efficientnet(model_class, shape) # A b1 pre-trained efficientnet is used\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    inp2 = Input(shape=(4,))\n",
    "    out_tab = tf.keras.layers.GaussianNoise(0.2)(inp2) # add some noise to our data\n",
    "    \n",
    "    x2 = Concatenate()([x, out_tab]) \n",
    "    \n",
    "    #### added ###\n",
    "    den_1 = Dense(20)(x2) # linear layer with 50 output nodes\n",
    "    den_1 = LeakyReLU(alpha=0.3)(den_1) # default alpha is 0.3\n",
    "    den_2 = Dense(500)(den_1)\n",
    "    den_2 = LeakyReLU(alpha=0.3)(den_2)\n",
    "    bn_1 = BatchNormalization()(den_2)\n",
    "    den_3 = Dense(100)(bn_1)\n",
    "    \n",
    "    x3 = Dropout(0.5)(den_3) # move the dropout layer to before the linear layers \n",
    "    \n",
    "    # the linear layers here are akin to the final dense layer usually used in a \"before concat\" model\n",
    "    out_1 = Dense(1)(x3)\n",
    "    out_2 = Dense(1, activation='relu')(x3)\n",
    "    y = out_1 + tf.keras.backend.cumsum(out_2, axis=1)\n",
    "    \n",
    "    model = Model([inp, inp2], y)\n",
    "    \n",
    "    # Take from kaggle  working output\n",
    "    weights = [w for w in os.listdir('./') if str(fold) in w][0] #use dir of './' if using training above, else can just use my trained weights in input\n",
    "    model.load_weights('./' + weights) #use dir of './' if using training above, else can just use my trained weights in input\n",
    "    \n",
    "    #take weights from uploaded weights\n",
    "    #uncomment when using notebook, instead of when committing notebook\n",
    "    #weights = [w for w in os.listdir('../input/scaled-weights-genderagepercentsmoker/') if str(fold) in w][0] #use dir of './' if using training above, else can just use my trained weights in input\n",
    "    #model.load_weights('../input/scaled-weights-genderagepercentsmoker/' + weights) #use dir of './' if using training above, else can just use my trained weights in input\n",
    "    return model\n",
    "#models = [build_pred_model(shape=(512, 512, 1), model_class='b1', fold=min_fold)]\n",
    "#uncomment the above to build model from the weights trained above, else can use the below code for building model\n",
    "models = [build_pred_model(shape=(512, 512, 1), model_class='b1', fold=min_fold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f9e3c52d583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m134\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "subs = []\n",
    "q = 0.5\n",
    "weeks = list(range(-12, 134))\n",
    "predictions = []\n",
    "for model in models:\n",
    "    metric = []\n",
    "    \n",
    "    A_test, B_test, P_test,W, FVC= {}, {}, {},{},{} \n",
    "    STD, WEEK = {}, {} \n",
    "    for p in test.Patient.unique():\n",
    "        x = [] \n",
    "        tab = [] \n",
    "        ldir = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/')\n",
    "        for i in ldir:\n",
    "            if int(i[:-4]) / len(ldir) < 0.8 and int(i[:-4]) / len(ldir) > 0.15: # what is this? only certain slices are being extracted for patients?\n",
    "                x.append(get_img(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/{i}')) \n",
    "                tab.append(get_tab_scaled(test.loc[test.Patient == p, :])) \n",
    "        if len(x) <= 1:\n",
    "            continue\n",
    "        print(len(test[test.Patient == p])) # number of fvc values to predict for every week\n",
    "        tab = np.array(tab) \n",
    "        print(\"number of patient data is: \", len(tab))\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        _a = model.predict([x, tab]) # Predict from all image data and tabular data.\n",
    "        a = np.quantile(_a, q) # Get the value at the 50th percentile\n",
    "        \n",
    "        A_test[p] = a\n",
    "        print(\"number of patient FVC data is: \", len(test.FVC.values[test.Patient == p])) # number of times patient takes\n",
    "        #######################################################################################################################################\n",
    "        B_test[p] = test.FVC.values[test.Patient == p] - a*test.Weeks.values[test.Patient == p] #to find the y intercept   #\n",
    "        #######################################################################################################################################\n",
    "        print(\"B_test[p] is: \", B_test[p])\n",
    "        #B_test is actually to find the intercept\n",
    "        \n",
    "        #P_test[p] = test.Percent_scld.values[test.Patient == p] \n",
    "        #print(\"P_test[p] is: \", P_test[p])\n",
    "        #WEEK[p] = test.baselined_week_scld.values[test.Patient == p]\n",
    "        #print(\"WEEK[p] is: \", WEEK[p])\n",
    "    \n",
    "    for p in test.Patient.unique():\n",
    "        for w in weeks:\n",
    "            patient_prediction = {}\n",
    "            fvc = A_test[p] * w + B_test[p]  #y = mx + c, A_test[p] = gradient of progression for patient p, w = week_num, B_test[p] is the calculated y_intercept\n",
    "            print(\"fvc is: \", fvc)\n",
    "            patient_prediction = {\n",
    "                'Week': w,\n",
    "                'Patient': p,\n",
    "                'FVC': np.sum(fvc)/len(fvc)\n",
    "            }\n",
    "            predictions.append(patient_prediction)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET RESULTS INTO A DATAFRAME \n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "# predictions_df.to_csv(\"test_predictions_GAPS_scaled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred_patients are unique patientID of patients in the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_patients  = predictions_df.Patient.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.rename(columns = {'Week': 'Weeks'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(predictions_df, test, on = ['Patient', 'Weeks'], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weeks</th>\n",
       "      <th>Patient</th>\n",
       "      <th>FVC_x</th>\n",
       "      <th>FVC_y</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "      <th>min_week</th>\n",
       "      <th>baselined_week</th>\n",
       "      <th>Percent_scld</th>\n",
       "      <th>Age_scld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3</td>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>1527.684593</td>\n",
       "      <td>1536</td>\n",
       "      <td>65.306122</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.261761</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>1500.614115</td>\n",
       "      <td>1368</td>\n",
       "      <td>58.163265</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.201730</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>1491.590622</td>\n",
       "      <td>1361</td>\n",
       "      <td>57.865646</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.199228</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>1482.567129</td>\n",
       "      <td>1465</td>\n",
       "      <td>62.287415</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.236391</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>ID00023637202179104603099</td>\n",
       "      <td>1473.543636</td>\n",
       "      <td>1681</td>\n",
       "      <td>71.471088</td>\n",
       "      <td>71</td>\n",
       "      <td>Female</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>-3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.313574</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>21</td>\n",
       "      <td>ID00421637202311550012437</td>\n",
       "      <td>2801.456878</td>\n",
       "      <td>2820</td>\n",
       "      <td>84.471603</td>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0.422836</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>23</td>\n",
       "      <td>ID00421637202311550012437</td>\n",
       "      <td>2794.842955</td>\n",
       "      <td>2853</td>\n",
       "      <td>85.460101</td>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.431144</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>29</td>\n",
       "      <td>ID00421637202311550012437</td>\n",
       "      <td>2775.001188</td>\n",
       "      <td>2716</td>\n",
       "      <td>81.356338</td>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0.396654</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>41</td>\n",
       "      <td>ID00421637202311550012437</td>\n",
       "      <td>2735.317654</td>\n",
       "      <td>2833</td>\n",
       "      <td>84.861011</td>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0.426109</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>54</td>\n",
       "      <td>ID00421637202311550012437</td>\n",
       "      <td>2692.327159</td>\n",
       "      <td>2771</td>\n",
       "      <td>83.003834</td>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Weeks                    Patient        FVC_x  FVC_y    Percent  Age  \\\n",
       "0       -3  ID00023637202179104603099  1527.684593   1536  65.306122   71   \n",
       "1        3  ID00023637202179104603099  1500.614115   1368  58.163265   71   \n",
       "2        5  ID00023637202179104603099  1491.590622   1361  57.865646   71   \n",
       "3        7  ID00023637202179104603099  1482.567129   1465  62.287415   71   \n",
       "4        9  ID00023637202179104603099  1473.543636   1681  71.471088   71   \n",
       "..     ...                        ...          ...    ...        ...  ...   \n",
       "307     21  ID00421637202311550012437  2801.456878   2820  84.471603   68   \n",
       "308     23  ID00421637202311550012437  2794.842955   2853  85.460101   68   \n",
       "309     29  ID00421637202311550012437  2775.001188   2716  81.356338   68   \n",
       "310     41  ID00421637202311550012437  2735.317654   2833  84.861011   68   \n",
       "311     54  ID00421637202311550012437  2692.327159   2771  83.003834   68   \n",
       "\n",
       "        Sex SmokingStatus  min_week  baselined_week  Percent_scld  Age_scld  \n",
       "0    Female     Ex-smoker        -3               0      0.261761  0.578947  \n",
       "1    Female     Ex-smoker        -3               6      0.201730  0.578947  \n",
       "2    Female     Ex-smoker        -3               8      0.199228  0.578947  \n",
       "3    Female     Ex-smoker        -3              10      0.236391  0.578947  \n",
       "4    Female     Ex-smoker        -3              12      0.313574  0.578947  \n",
       "..      ...           ...       ...             ...           ...       ...  \n",
       "307    Male     Ex-smoker        15               6      0.422836  0.500000  \n",
       "308    Male     Ex-smoker        15               8      0.431144  0.500000  \n",
       "309    Male     Ex-smoker        15              14      0.396654  0.500000  \n",
       "310    Male     Ex-smoker        15              26      0.426109  0.500000  \n",
       "311    Male     Ex-smoker        15              39      0.410500  0.500000  \n",
       "\n",
       "[312 rows x 12 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt((1/len(merged_data)) * sum(np.square(merged_data.FVC_x - merged_data.FVC_y)))  #change this part for non_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rmse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-76fe0720390e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 144.413 (for pyotrch part 2 rmse)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# around 142 for this version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rmse' is not defined"
     ]
    }
   ],
   "source": [
    "# 144.413 (for pyotrch part 2 rmse)\n",
    "# around 142 for this version\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmse for unscaled notebook with percent included = 143.2077201851251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
